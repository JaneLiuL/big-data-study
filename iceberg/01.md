# spark 对接 iceberg

确保spark跟iceberg版本兼容之后
需要增加iceberg依赖配置
1）Spark Shell 或提交任务时指定依赖
```bash
# Spark Shell 方式（local 模式）
spark-shell \
  --packages org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.3.1 \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  --conf spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog \
  --conf spark.sql.catalog.spark_catalog.type=hive \
  --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
  --conf spark.sql.catalog.local.type=hadoop \
  --conf spark.sql.catalog.local.warehouse=/path/to/iceberg/warehouse
```

2）集群环境配置（spark-defaults.conf）
在 spark-defaults.conf 中添加全局配置：
```
# 启用 Iceberg 扩展
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# 配置 Hive  metastore 作为 catalog（推荐生产环境）
spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type=hive

# 配置本地文件系统作为 catalog（测试用）
spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.local.type=hadoop
spark.sql.catalog.local.warehouse=/path/to/iceberg/warehouse
```

表迁移：可通过 INSERT INTO iceberg_table SELECT * FROM hive_table 将 Hive 表数据迁移到 Iceberg。
兼容性：Iceberg 表可与 Hive、Trino 等其他引擎共享，实现多引擎协作。
元数据维护：定期执行 VACUUM 清理过期快照（避免元数据膨胀）：
sql
VACUUM spark_catalog.default.iceberg_test RETAIN 7 DAYS;  -- 保留最近7天的快照


***spark 默认覆盖模式是静态的，写入iceberg的时候建议使用动态覆盖***

iceberg不会物理删除数据
Iceberg 的快照机制决定了用户在删除数据时，并不会立即将数据从存储上删除，而是**写入新的 delete file，用于记录被删除的数据。delete file 分为 position delete file 和 equality delete file**，前者记录某个 data file 的某一行被删除的信息，后者记录某个 key 的值被删除，通常用在 upsert 写入场景。
此外，Iceberg 提供了**VACUUM 命令来清理表中被标记为删除的数据** ，只有在执行 VACUUM 命令后，满足相应条件的数据才会被真正物理删除。
