
## 实践：用 Spark 读写 Parquet/ORC 文件到 MinIO，验证大文件（GB 级）和小文件（百万级）场景的稳定性
测试场景：
* 大文件场景：生成约 5GB 数据（约 500 万行），测试批量读写性能
* 小文件场景：生成 10 万个小文件（每个文件 1 行数据），测试大量小文件的处理能力
核心功能：
* 自动生成结构化测试数据（包含多种数据类型，每行约 1KB）
* 分别测试 Parquet 和 ORC 两种列存格式
* 记录写入时间、读取时间和查询时间
* 验证数据完整性（对比读写前后的行数）
安装 PySpark：pip install pyspark
```bash
spark-submit --master yarn spark_minio_test.py
# 或本地模式（仅用于测试，不适合大文件场景）
spark-submit spark_minio_test.py
```
大文件场景：
* 观察是否出现内存溢出（OOM）
* 检查写入过程中是否有数据块损坏
* 验证读取时是否能完整恢复数据
小文件场景：
* 观察 Spark 任务调度是否正常（避免任务数量过多导致崩溃）
* 检查 MinIO 元数据服务是否稳定（大量小文件会增加元数据压力）
* 记录查询延迟变化（小文件通常会导致查询变慢）


## 实践：用 Trino 查询 MinIO 中的 CSV/Parquet 数据，测试并发查询性能
todo


## 实践：创建 Iceberg 表，写入数据到 MinIO，测试快照、时间旅行等功能
todo